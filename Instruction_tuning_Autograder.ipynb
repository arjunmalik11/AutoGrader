{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "43a11b105f0f43c2a88b9790868e5e3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8005669954fa47ea907fdfbafa75d406",
              "IPY_MODEL_c1a14ea046ff4a51a3eb79b3ef831cf6",
              "IPY_MODEL_5a97e3ef5fd7406c8df86f54b312d5c6"
            ],
            "layout": "IPY_MODEL_562e78b0d4f14e5c9cbc838af6535de6"
          }
        },
        "8005669954fa47ea907fdfbafa75d406": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e1f23f02e6442068462fc223a37d8f0",
            "placeholder": "​",
            "style": "IPY_MODEL_f7d65dd8215c458bbb3fc2f366d675f6",
            "value": "Downloading data files: 100%"
          }
        },
        "c1a14ea046ff4a51a3eb79b3ef831cf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f900b758ad545989f19d2045bc47b58",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e1a37027c298455f8ddfb2bcbd76d1c3",
            "value": 1
          }
        },
        "5a97e3ef5fd7406c8df86f54b312d5c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44b8fe96f9cc40e6bee649730822fa4b",
            "placeholder": "​",
            "style": "IPY_MODEL_257bbb647ac84408af66e27499667670",
            "value": " 1/1 [00:00&lt;00:00, 40.79it/s]"
          }
        },
        "562e78b0d4f14e5c9cbc838af6535de6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e1f23f02e6442068462fc223a37d8f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7d65dd8215c458bbb3fc2f366d675f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8f900b758ad545989f19d2045bc47b58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1a37027c298455f8ddfb2bcbd76d1c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "44b8fe96f9cc40e6bee649730822fa4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "257bbb647ac84408af66e27499667670": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e8e340dc3d1b4376947b31a7cfa4086a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_575168a0423e48abbabd2e54af71d5e3",
              "IPY_MODEL_39b853e2b7c24a7bb2f03301a222d609",
              "IPY_MODEL_a00c918904984645837d30bc67b6ecfb"
            ],
            "layout": "IPY_MODEL_56a1bf968d1343e680052169524283b5"
          }
        },
        "575168a0423e48abbabd2e54af71d5e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02f56626db864bba85d251caebe07b00",
            "placeholder": "​",
            "style": "IPY_MODEL_8c602a5e803b40b180c3d62c92912d38",
            "value": "Extracting data files: 100%"
          }
        },
        "39b853e2b7c24a7bb2f03301a222d609": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_118b5027d81e42e9ac23625d8e4ea77c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_667f536e1aa045cca2dfb32cb6aa9e64",
            "value": 1
          }
        },
        "a00c918904984645837d30bc67b6ecfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_489b9643efe0454ba1c49952c376ae96",
            "placeholder": "​",
            "style": "IPY_MODEL_75ca5e63ddfc4ec783d7a305de4113d3",
            "value": " 1/1 [00:00&lt;00:00, 33.44it/s]"
          }
        },
        "56a1bf968d1343e680052169524283b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02f56626db864bba85d251caebe07b00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c602a5e803b40b180c3d62c92912d38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "118b5027d81e42e9ac23625d8e4ea77c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "667f536e1aa045cca2dfb32cb6aa9e64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "489b9643efe0454ba1c49952c376ae96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75ca5e63ddfc4ec783d7a305de4113d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ce030757343478e865a1c896d30933e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0d194fc561f3405187203d333951b5d2",
              "IPY_MODEL_b899416e88dc42ab907d1600b1168e24",
              "IPY_MODEL_fb486429e5f948d9acd0aeddd2b8b203"
            ],
            "layout": "IPY_MODEL_985ebccab81e4295a32255dd0f81a33a"
          }
        },
        "0d194fc561f3405187203d333951b5d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f803073a22148d89ff2d7b94955fc52",
            "placeholder": "​",
            "style": "IPY_MODEL_7a34c25705e242f58383e6221eb28206",
            "value": "Generating train split: "
          }
        },
        "b899416e88dc42ab907d1600b1168e24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9001f18a81044db893b99cb98a19884",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_63938bb204b84a0db9e11fc6a340592a",
            "value": 1
          }
        },
        "fb486429e5f948d9acd0aeddd2b8b203": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38888edd28d64b67a2321f7504906570",
            "placeholder": "​",
            "style": "IPY_MODEL_49f52fe9cafa40e792afc4b2508e978a",
            "value": " 5/0 [00:00&lt;00:00, 99.82 examples/s]"
          }
        },
        "985ebccab81e4295a32255dd0f81a33a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f803073a22148d89ff2d7b94955fc52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a34c25705e242f58383e6221eb28206": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d9001f18a81044db893b99cb98a19884": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "63938bb204b84a0db9e11fc6a340592a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "38888edd28d64b67a2321f7504906570": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49f52fe9cafa40e792afc4b2508e978a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b103e6df90f24139ae0981e1acaaf807": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_54c6aef313724c7ba26545d634a7a4d4",
              "IPY_MODEL_bf1a850bfaf5415c874e477043d13fc2",
              "IPY_MODEL_34f90e61e0fd45fbae16ce3079302f3a"
            ],
            "layout": "IPY_MODEL_3a24a541ec964e98a5a4ca148f071c99"
          }
        },
        "54c6aef313724c7ba26545d634a7a4d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a94a00de33aa4b6883338985d364f642",
            "placeholder": "​",
            "style": "IPY_MODEL_6bd55f651a4546c992e2710fd880659c",
            "value": "Map: 100%"
          }
        },
        "bf1a850bfaf5415c874e477043d13fc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2309d2aeef2f41ea9e0ddea3c2c0193d",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4760844f9c09401ca3f202f247e432c3",
            "value": 5
          }
        },
        "34f90e61e0fd45fbae16ce3079302f3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee50f327eb2d4cc78d28ca7bb147aa3f",
            "placeholder": "​",
            "style": "IPY_MODEL_374535d80ad94ad287effeab98a65f5a",
            "value": " 5/5 [00:00&lt;00:00, 19.35 examples/s]"
          }
        },
        "3a24a541ec964e98a5a4ca148f071c99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a94a00de33aa4b6883338985d364f642": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bd55f651a4546c992e2710fd880659c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2309d2aeef2f41ea9e0ddea3c2c0193d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4760844f9c09401ca3f202f247e432c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ee50f327eb2d4cc78d28ca7bb147aa3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "374535d80ad94ad287effeab98a65f5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2O9IWcdoI599",
        "outputId": "bd2cf40b-148d-4997-9030-80e6f3a2858e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jsonlines in /usr/local/lib/python3.10/dist-packages (4.0.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines) (23.1.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# Credits for the code DeepLearning.AI\n",
        "!pip install jsonlines\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install accelerate -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "es7fIW31exgL",
        "outputId": "a8654145-0fcc-47ec-9701-1d568d79adab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.24.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.19.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install lamini"
      ],
      "metadata": {
        "id": "xuIJ8tO5o599"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import jsonlines\n",
        "\n",
        "from datasets import load_dataset\n",
        "from pprint import pprint\n",
        "\n",
        "# from llama import BasicModelRunner\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer,Trainer"
      ],
      "metadata": {
        "id": "gR2Rvw6DI9S6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df=pd.read_excel(\"/content/Autograde_Dataset.xlsx\")\n",
        "df=df.fillna(\"Overall Feedback\\nNice report!\")\n",
        "df=df.drop(\"Homework\",axis=1)\n",
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfpLZsJAHsgj",
        "outputId": "912748d7-5294-41f3-ec62-9a7ed0839e8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Task', 'Task_Solution_Code', 'Task_Solution_Report',\n",
              "       'Task_Solution_Expectations', 'Task_Solution_Result_Points',\n",
              "       'Task_Solution_Result_Feedback'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "instruction_tuned_dataset = df#load_dataset(\"squad_v2\", split=\"train\")#, streaming=True)"
      ],
      "metadata": {
        "id": "zyglG3tjMd9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load_dataset(\"squad_v2\")"
      ],
      "metadata": {
        "id": "UmhMJv0o0Sm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# m = 1200\n",
        "# print(\"Instruction-tuned dataset:\")\n",
        "top_m = df#list(itertools.islice(instruction_tuned_dataset, m))\n",
        "# for j in top_m:\n",
        "#   print(top_m[j])"
      ],
      "metadata": {
        "id": "wKEqj57RMfCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# top_m"
      ],
      "metadata": {
        "id": "SGKjJ7V3cU96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt= \"\"\" Given the task{task}\\ncode{code}\\nreport{report}\\ntask_expectations{task_expectations}\\n\n",
        "Provide a feedback:\n",
        "\"\"\"\n",
        "\n",
        "# prompt_template_without_input = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "# ### Instruction:\n",
        "# {instruction}\n",
        "\n",
        "# ### Response:\"\"\""
      ],
      "metadata": {
        "id": "X7TsssEWpymq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_data = []\n",
        "for j in range(len(top_m)):\n",
        "  processed_prompt = prompt.format(task=df[\"Task\"][j],code=df[\"Task_Solution_Code\"][j],report=df[\"Task_Solution_Report\"][j],\n",
        "                                   task_expectations=df[\"Task_Solution_Expectations\"][j])\n",
        "  # else:\n",
        "    # processed_prompt = prompt_template_with_input.format(instruction=j[\"instruction\"], input=j[\"input\"])\n",
        "\n",
        "  processed_data.append({\"input\": processed_prompt, \"output\": df[\"Task_Solution_Result_Feedback\"][j]})\n"
      ],
      "metadata": {
        "id": "zCb94aUfp9PN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# processed_data[0]"
      ],
      "metadata": {
        "id": "ktj2ASNAM2Pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with jsonlines.open(f'my_squad.jsonl', 'w') as writer:\n",
        "    writer.write_all(processed_data)"
      ],
      "metadata": {
        "id": "YI-_042tI9jP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset_path_hf = \"lamini/alpaca\"\n",
        "# dataset_hf = load_dataset(dataset_path_hf)\n",
        "# print(dataset_hf)"
      ],
      "metadata": {
        "id": "RT6zUXDrI9oZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# non_instruct_model = BasicModelRunner(\"meta-llama/Llama-2-7b-hf\")\n",
        "# non_instruct_output = non_instruct_model(\"Tell me how to train my dog to sit\")\n",
        "# print(\"Not instruction-tuned output (Llama 2 Base):\", non_instruct_output)"
      ],
      "metadata": {
        "id": "eJQ5uU9VvqiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\") #(\"EleutherAI/pythia-70m\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-70m\")#(\"EleutherAI/pythia-70m\")\n",
        "model_name=\"EleutherAI/pythia-70m\""
      ],
      "metadata": {
        "id": "7dX7PHDawaMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(text, model, tokenizer, max_input_tokens=514, max_output_tokens=100):\n",
        "  # Tokenize\n",
        "  input_ids = tokenizer.encode(\n",
        "          text,\n",
        "          return_tensors=\"pt\",\n",
        "          truncation=True,\n",
        "          max_length=max_input_tokens\n",
        "  )\n",
        "\n",
        "  # Generate\n",
        "  device = model.device\n",
        "  generated_tokens_with_prompt = model.generate(\n",
        "    input_ids=input_ids.to(device),\n",
        "    max_length=max_output_tokens\n",
        "  )\n",
        "\n",
        "  # Decode\n",
        "  generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
        "\n",
        "  # Strip the prompt\n",
        "  generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
        "\n",
        "  return generated_text_answer"
      ],
      "metadata": {
        "id": "55CtX6BSwbgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetuning_dataset_path = \"squad_v2\"\n",
        "finetuning_dataset = load_dataset(finetuning_dataset_path)\n",
        "print(finetuning_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMLVdct5wbuY",
        "outputId": "a373028f-f655-40cf-a0b0-3fce08ac1511"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
            "        num_rows: 130319\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
            "        num_rows: 11873\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_sample = finetuning_dataset[\"validation\"][0]\n",
        "print(test_sample[\"question\"])\n",
        "\n",
        "print(inference(test_sample[\"question\"], model, tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sarn-W6Ywb6j",
        "outputId": "421ba9de-1c76-4972-c357-a24defcc830c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In what country is Normandy located?\n",
            "\n",
            "\n",
            "The only way to get to Normandy is to get to the top of the mountain.\n",
            "\n",
            "The only way to get to Normandy is to get to the top of the mountain.\n",
            "\n",
            "The only way to get to Normandy is to get to the top of the mountain.\n",
            "\n",
            "The only way to get to Normandy is to get to the top of the mountain.\n",
            "\n",
            "The only way to get to Normandy is to\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pssst! If you were curious how to upload your own dataset to Huggingface\n",
        "# Here is how we did it\n",
        "\n",
        "# !pip install huggingface_hub\n",
        "# !huggingface-cli login\n",
        "\n",
        "# import pandas as pd\n",
        "# import datasets\n",
        "# from datasets import Dataset\n",
        "\n",
        "# finetuning_dataset = Dataset.from_pandas(pd.DataFrame(data=finetuning_dataset))\n",
        "# finetuning_dataset.push_to_hub(dataset_path_hf)"
      ],
      "metadata": {
        "id": "cAG6j52lwcGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def tokenize_function(examples):\n",
        "#     text = examples[\"input\"][0] + examples[\"output\"][0][\"text\"][0]\n",
        "\n",
        "\n",
        "#     tokenizer.pad_token = tokenizer.eos_token\n",
        "#     tokenized_inputs = tokenizer(\n",
        "#         text,\n",
        "#         return_tensors=\"np\",\n",
        "#         padding=True,\n",
        "#     )\n",
        "\n",
        "#     max_length = min(\n",
        "#         tokenized_inputs[\"input_ids\"].shape[1],\n",
        "#         2048\n",
        "#     )\n",
        "#     tokenizer.truncation_side = \"right\"\n",
        "#     tokenized_inputs = tokenizer(\n",
        "#         text,\n",
        "#         return_tensors=\"np\",\n",
        "#         truncation=True,\n",
        "#         max_length=2048,\n",
        "#         padding=\"max_length\"\n",
        "#     )\n",
        "\n",
        "#     return tokenized_inputs\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    # Combine context and question for input\n",
        "    text_input = examples[\"input\"][0]\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "    # Tokenize input (context + question)\n",
        "    tokenized_inputs = tokenizer(\n",
        "        text_input,\n",
        "        return_tensors=\"np\",\n",
        "        truncation=True,\n",
        "        max_length=2048,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "    # Tokenize output (answer)\n",
        "    text_output = examples[\"output\"][0]\n",
        "    tokenized_output = tokenizer(\n",
        "        text_output,\n",
        "        return_tensors=\"np\",\n",
        "        truncation=True,\n",
        "        max_length=2048,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "    # Update tokenized_inputs with tokenized_output for labels\n",
        "    tokenized_inputs[\"labels\"] = tokenized_output[\"input_ids\"]\n",
        "\n",
        "    return tokenized_inputs\n",
        "\n"
      ],
      "metadata": {
        "id": "wcmP143KwcTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install utilities"
      ],
      "metadata": {
        "id": "2P_JfugUmNlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "import tempfile\n",
        "import logging\n",
        "import random\n",
        "# import config\n",
        "import os\n",
        "import yaml\n",
        "import time\n",
        "import torch\n",
        "import transformers\n",
        "import pandas as pd\n",
        "import jsonlines\n",
        "\n",
        "# from utilities import *\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForCausalLM\n",
        "from transformers import TrainingArguments\n",
        "from transformers import AutoModelForCausalLM\n",
        "# from llama import BasicModelRunner\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "global_config = None"
      ],
      "metadata": {
        "id": "IXck4UemmH_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# s=5\n",
        "# sample_5 = list(itertools.islice(instruction_tuned_dataset, s))\n",
        "# for j in sample_5:\n",
        "#   print(j)\n",
        "finetuning_dataset_loaded = datasets.load_dataset(\"json\", data_files=\"/content/my_squad.jsonl\", split=\"train\")"
      ],
      "metadata": {
        "id": "xvKj41ws-ngf",
        "outputId": "62baa97b-296c-46bf-8365-1cb2fe24151e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "43a11b105f0f43c2a88b9790868e5e3d",
            "8005669954fa47ea907fdfbafa75d406",
            "c1a14ea046ff4a51a3eb79b3ef831cf6",
            "5a97e3ef5fd7406c8df86f54b312d5c6",
            "562e78b0d4f14e5c9cbc838af6535de6",
            "2e1f23f02e6442068462fc223a37d8f0",
            "f7d65dd8215c458bbb3fc2f366d675f6",
            "8f900b758ad545989f19d2045bc47b58",
            "e1a37027c298455f8ddfb2bcbd76d1c3",
            "44b8fe96f9cc40e6bee649730822fa4b",
            "257bbb647ac84408af66e27499667670",
            "e8e340dc3d1b4376947b31a7cfa4086a",
            "575168a0423e48abbabd2e54af71d5e3",
            "39b853e2b7c24a7bb2f03301a222d609",
            "a00c918904984645837d30bc67b6ecfb",
            "56a1bf968d1343e680052169524283b5",
            "02f56626db864bba85d251caebe07b00",
            "8c602a5e803b40b180c3d62c92912d38",
            "118b5027d81e42e9ac23625d8e4ea77c",
            "667f536e1aa045cca2dfb32cb6aa9e64",
            "489b9643efe0454ba1c49952c376ae96",
            "75ca5e63ddfc4ec783d7a305de4113d3",
            "6ce030757343478e865a1c896d30933e",
            "0d194fc561f3405187203d333951b5d2",
            "b899416e88dc42ab907d1600b1168e24",
            "fb486429e5f948d9acd0aeddd2b8b203",
            "985ebccab81e4295a32255dd0f81a33a",
            "8f803073a22148d89ff2d7b94955fc52",
            "7a34c25705e242f58383e6221eb28206",
            "d9001f18a81044db893b99cb98a19884",
            "63938bb204b84a0db9e11fc6a340592a",
            "38888edd28d64b67a2321f7504906570",
            "49f52fe9cafa40e792afc4b2508e978a"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "43a11b105f0f43c2a88b9790868e5e3d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e8e340dc3d1b4376947b31a7cfa4086a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6ce030757343478e865a1c896d30933e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "finetuning_dataset_loaded[\"output\"][0]"
      ],
      "metadata": {
        "id": "BPrUYgm4_cc7",
        "outputId": "f3cfa276-cec2-4250-937f-38d8c9bdabd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Overall Feedback\\nNice report!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# finetuning_dataset_loaded = datasets.load_dataset(\"json\", data_files=\"squad_v2\", split=\"train\")\n",
        "\n",
        "tokenized_dataset = finetuning_dataset_loaded.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    batch_size=1,\n",
        "    drop_last_batch=True\n",
        ")\n",
        "\n",
        "print(tokenized_dataset)"
      ],
      "metadata": {
        "id": "IJwCke7wwcf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123,
          "referenced_widgets": [
            "b103e6df90f24139ae0981e1acaaf807",
            "54c6aef313724c7ba26545d634a7a4d4",
            "bf1a850bfaf5415c874e477043d13fc2",
            "34f90e61e0fd45fbae16ce3079302f3a",
            "3a24a541ec964e98a5a4ca148f071c99",
            "a94a00de33aa4b6883338985d364f642",
            "6bd55f651a4546c992e2710fd880659c",
            "2309d2aeef2f41ea9e0ddea3c2c0193d",
            "4760844f9c09401ca3f202f247e432c3",
            "ee50f327eb2d4cc78d28ca7bb147aa3f",
            "374535d80ad94ad287effeab98a65f5a"
          ]
        },
        "outputId": "cdd7e594-5d09-4323-f68d-56164e8a0b7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b103e6df90f24139ae0981e1acaaf807"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['input', 'output', 'input_ids', 'attention_mask', 'labels'],\n",
            "    num_rows: 5\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenized_dataset = tokenized_dataset.add_column(\"labels\", tokenized_dataset[\"input_ids\"])\n",
        "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, shuffle=True, seed=123)\n",
        "print(split_dataset)"
      ],
      "metadata": {
        "id": "I__sgj_wx3yk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a871d55-84e5-482a-c5f7-a7516f19201a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input', 'output', 'input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 4\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['input', 'output', 'input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 1\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = \"my_squad.jsonl\"\n",
        "dataset_path = f\"/content/{dataset_name}\"\n",
        "use_hf = False"
      ],
      "metadata": {
        "id": "Oka6iLtZx4Bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name=\"deepset/tinyroberta-squad2\""
      ],
      "metadata": {
        "id": "ikGkjHFsx4Nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_config = {\n",
        "    \"model\": {\n",
        "        \"pretrained_name\": model_name,\n",
        "        \"max_length\" : 2048\n",
        "    },\n",
        "    \"datasets\": {\n",
        "        \"use_hf\": use_hf,\n",
        "        \"path\": dataset_path\n",
        "    },\n",
        "    \"verbose\": True\n",
        "}"
      ],
      "metadata": {
        "id": "Lq4Vc0nQx4eK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "# train_dataset, test_dataset = tokenize_and_split_data(training_config, tokenizer)\n",
        "train_dataset, test_dataset =split_dataset\n",
        "print(train_dataset)\n",
        "print(test_dataset)"
      ],
      "metadata": {
        "id": "KNkP6gBKwctM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f47e2a4-72fe-4f62-b407-5f3b068a0eaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train\n",
            "test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "oOi4ou09E14O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcb99152-fbbc-4e2a-fb05-04804d9dec23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at deepset/tinyroberta-squad2 and are newly initialized: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device_count = torch.cuda.device_count()\n",
        "if device_count > 0:\n",
        "    logger.debug(\"Select GPU device\")\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    logger.debug(\"Select CPU device\")\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "Fp8PwVd-E2H8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.to(device)"
      ],
      "metadata": {
        "id": "vxGFHXVbE2Mr",
        "outputId": "b2a4c2cc-0d97-447a-808c-f817e6cb6bb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaForCausalLM(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (lm_head): RobertaLMHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (decoder): Linear(in_features=768, out_features=50265, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split_dataset[test_dataset][0]['output']\n",
        "#split_dataset[test_dataset][0]['output']['text'][0]"
      ],
      "metadata": {
        "id": "s0FNePP1F21V",
        "outputId": "30b7ff94-7441-4f2a-d1bf-4a981c3f2f0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Overall Feedback\\n-2 (Report is missing discussions on improvements or degradations of accuracy when you tried updating create_better_vectors)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_text = split_dataset[test_dataset][0]['input']\n",
        "print(\"Question input (test):\", test_text)\n",
        "print(f\"Correct answer : {split_dataset[test_dataset][0]['output']}\")\n",
        "print(\"Model's answer: \")\n",
        "print(inference(test_text, base_model, tokenizer))"
      ],
      "metadata": {
        "id": "GH3rjhWtE2Rc",
        "outputId": "1f5e94e9-8701-4586-e71d-0a852b5a9597",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question input (test):  Given the taskCS 590 NLP\n",
            "HW5, Word2Vec\n",
            "Due 10/30 11:59 pm\n",
            "Overall Goal:\n",
            "In this homework you will work with word2vec embeddings to understand them and set up a simple\n",
            "classifier. To save time, you will be again working with the dataset from HW3-4.\n",
            "Useful python packages:\n",
            "For word2vec, you may find the gensim package useful:\n",
            "https://radimrehurek.com/gensim/models/word2vec.html\n",
            "Note that gensim has built in pretrained word2vec embeddings so you should choose 1 or experiment\n",
            "with multiple in this homework.\n",
            "(https://radimrehurek.com/gensim/models/word2vec.html#pretrained-models)\n",
            "Or use glove embeddings (might require different steps to read them in):\n",
            "https://nlp.stanford.edu/projects/glove/\n",
            "Gensim also has nice built-in functions for working with word2vec data.\n",
            "Another option would be to use Polyglot for embeddings:\n",
            "https://polyglot.readthedocs.io/en/latest/Embeddings.html\n",
            "Any other options should be cleared with the professor first.\n",
            "Dataset\n",
            "The dataset that will be used is the IMDB sentiment dataset used in HW3. This consists of positive and\n",
            "negative movie reviews.\n",
            "The data can be found on brightspace or here:\n",
            "https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
            "The .csv file contains a column of the review and a column for positive or negative. For python,\n",
            "modules like the csv module (https://docs.python.org/3/library/csv.html) can be useful for reading in\n",
            "csv files (you may use others like numpy as well).\n",
            "Take time to understand how the data is laid out.\n",
            "Word2Vec Tasks\n",
            "You will be creating and then updating a simple sentiment classifier based on word embeddings. Your\n",
            "aim is to work with embeddings to understand them a bit further. You may use any pretrained\n",
            "word2vec, or Glove embeddings you choose. Note: This assignment is to practice working with\n",
            "pretrained embeddings, so you should not train any of your own.\n",
            "create_sentiment_vectors(train_file)\n",
            "This method takes in a train file and produces “sentiment vectors” for each label (negative, positive)\n",
            "in the train file. This train file should be 80% of the original IMDB_Dataset (if you successfully\n",
            "implemented split_data() in hw4, you can use those output files). By this point you should have an\n",
            "idea of what preprocessing you should start with, so that is left up to you.\n",
            "The sentiment vectors are produced by taking an element-wise average for all words within the\n",
            "training texts for each label. Specifically, the vectors are created as follows:\n",
            "1. Get the unique vocabulary (types) for all texts labeled positive.\n",
            "2. Obtain the pretrained embedding for each type.\n",
            "3. Calculate a positive sentiment vector by averaging all the embeddings obtained.\n",
            "4. Repeat for negative sentiment.\n",
            "The method then returns a dictionary in the form {‘positive’: [0.2...-0.5], ‘negative’:[0.1...0.2]}. I.e.,\n",
            "each vector produced has a corresponding entry which is the label.\n",
            "predict_sentiment(test_file, sent_vects)\n",
            "This method takes in the test file (20% of IMDB_Dataset) and the produced sent_vects dictionary\n",
            "(from create_sentiment_vectors) and makes simple predictions for all test texts, as follows:\n",
            "1. For each test text,\n",
            "a. Calculate an embedding to represent the test text (average of individual word\n",
            "embeddings)\n",
            "b. Calculate cosine similarity between the test embedding and each vector in the\n",
            "sent_vects.\n",
            "c. Choose the more similar (highest scoring) label as the predicted label.\n",
            "You do not need to output to a file for this function, but you should keep track of accuracy by noting\n",
            "the number of predictions your methods got correct versus total prediction. The function should print\n",
            "out the final accuracy.\n",
            "create_better_vectors(train_file)\n",
            "This method is similar to create_sentiment_vectors, but now you are trying to create embeddings\n",
            "which more accurately represent the sentiments. You should try different additions to the\n",
            "create_sentiment_vectors (BUT THIS SHOULD BE A SEPARATE FUNCTION!) to improve the vectors\n",
            "which represent sentiment vectors and improve the accuracy of your simple classification algorithm.\n",
            "Note: Purposely leaving out preprocessing in the first function and then adding it in this function is\n",
            "not improvement. You will not receive points for this.\n",
            "predict_better(test_file, sent_vects)\n",
            "Again, similar to the original predict_sentiment, but this should match the changes made in better\n",
            "vectors for consistency. For example, if you remove all stop words in create_better_vectors, then\n",
            "you’ll want to also do that in predict_better.\n",
            "Coding Requirement\n",
            "• Your functions should have the same name and number of arguments as noted above.\n",
            "• The preprocessing in each train function, should be used consistently in the corresponding\n",
            "predict functions.\n",
            "• All 4 functions should be separate.\n",
            "• You must use pretrained embeddings, either w2v or Glove, you should note which you use.\n",
            "• Good documentation is always required for understanding of code. Make sure to include\n",
            "explanation of coding steps.\n",
            "• You should use the same consistent train/test splits for all changes/comparisons or else your\n",
            "comparisons will be compromised.\n",
            "Report Requirement\n",
            "Include in your report:\n",
            "• The standard preprocessing used in all 4 methods.\n",
            "• The pretrained embeddings used (link to external source).\n",
            "• The improvements tried in create_better_vectors.\n",
            "• A table containing the accuracy for the first 2 methods (create_sentiment and\n",
            "predict_sentiment) as well as the improved accuracies achieved.\n",
            "o Each accuracy must be clearly marked.\n",
            "o Add this table early on and note down improvements or degradations of accuracy\n",
            "when you tried updating create_better_vectors.\n",
            "• Discussion on the results achieve.\n",
            "o Surprises, difficulties, specific additions which helped improve accuracy or hurt\n",
            "accuracy\n",
            "Note: Like all homeworks, you should not screenshot results. You should put them into a nice table\n",
            "in your report for clarity.\n",
            "Final Report\n",
            "Just like in the previous homeworks, you will be documenting observations and results in a final\n",
            "report. This is the best way to show your thought process throughout the homework, so it is best to\n",
            "update it as you go along and then refine at the end, rather than try and write all at the end.\n",
            "Additional Tips/Guidance\n",
            "1. Your thoughtfulness in approaching and analyzing your models will determine the final grade\n",
            "for this homework. This means that lack in analysis will receive lower scores as indicated in\n",
            "the grading scale.\n",
            "2. Note that preprocessing the text is up to you. This assignment is not specifically evaluating\n",
            "preprocessing like the first assignment, but here you have a chance to practice any\n",
            "preprocessing for your final project and future assignments. (You should be doing some\n",
            "preprocessing, or else you’ll be purposely setting your models up to fail.)\n",
            "3. Get familiar with the gensim library on your own on small sets of data. Just copying and\n",
            "pasting the code without fully understanding it will end up being detrimental as you may not\n",
            "be able to accurately give proper analysis and observations of results.\n",
            "4. DO NOT SHARE CODE. I have linked gensim libraries which will be useful for this assignment.\n",
            "You may also come to me to discuss/figure things out.\n",
            "5. Start the assignment early. You will have at least 2 weeks to complete this assignment, which\n",
            "is plenty of time if you start early any familiarize yourself with the libraries. IF YOU WAIT\n",
            "UNTIL THE LAST MINUTE TO START, YOU WILL BE LESS LIKELY TO DO WELL. I won’t be\n",
            "granting any additional extensions so each late assignment with follow the late grade policy.\n",
            "6. Ask questions/approach like a researcher. Think like this is your chance to explore NLP models\n",
            "and analyze their effectiveness.\n",
            "Grading\n",
            "Assignment will be graded as follows:\n",
            "Description Points\n",
            "Code Runs 5\n",
            "create_* functions 15\n",
            "predict_* functions 10\n",
            "Required Report items 15\n",
            "Documentation (Comments, functions, etc) 5\n",
            "Total: 50\n",
            "codeimport pandas as pd #pandas for datframe operations\n",
            "import numpy as np # numpy for vector calculations\n",
            "from sklearn.model_selection import train_test_split #For splitting data into train and test set\n",
            "from sklearn.metrics.pairwise import cosine_similarity # measuring distance between vectors\n",
            "import gensim # for pretrained embeddings\n",
            "import nltk #natural language toolkit\n",
            "from nltk.tokenize import word_tokenize # each token is a word separared by space\n",
            "from nltk.corpus import stopwords # Eg- the, of etc\n",
            "from nltk.stem import PorterStemmer # keeping stem words\n",
            "nltk.download('punkt') #nltk toolkit\n",
            "nltk.download('stopwords') # list of stopwords in english\n",
            "import gensim.downloader #to download pretrained embeddings\n",
            "\n",
            "# Define a function to split the data into train and test sets\n",
            "def split_data(path_to_data_file, test_percentage): # 2 parameters- data file path and percentage of data to be taken for testing\n",
            "    data = pd.read_csv(path_to_data_file,encoding='utf-8') # reading file\n",
            "    train, test = train_test_split(data, test_size=test_percentage/100, stratify=data['sentiment']) # for splitting\n",
            "    #in train test. random_state sets seed to a value to control randomness and stratify ensures that samples from both class label are taken\n",
            "    # proportionally to avoid class imbalance in train and test data.\n",
            "    train_file_name = f\"train_{path_to_data_file.split('.')[0]}_{test_percentage}.csv\" #naming train data\n",
            "    test_file_name = f\"test_{path_to_data_file.split('.')[0]}_{test_percentage}.csv\"   # naming test data\n",
            "\n",
            "    train.to_csv(train_file_name, index=False) # write train file to csv\n",
            "    test.to_csv(test_file_name, index=False) # write test file to csv\n",
            "\n",
            "split_data(\"IMDB_Dataset.csv\",20) # calling function with test percentage of 20%\n",
            "\n",
            "# Loading pretrained Word2Vec model\n",
            "model=gensim.downloader.load('word2vec-google-news-300') #google's word2vec embedding\n",
            "model.save(\"word2vec_model\") #saving model locally\n",
            "\n",
            "\n",
            "def preprocess_text(text):\n",
            "    # Tokenize the text\n",
            "    tokens = word_tokenize(text)\n",
            "\n",
            "    # Remove punctuation and convert to lowercase\n",
            "    tokens = [word.lower() for word in tokens if word.isalnum()]\n",
            "\n",
            "    # Remove stop words\n",
            "    # tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
            "\n",
            "    # Handle contractions\n",
            "    tokens = handle_contractions(tokens)\n",
            "\n",
            "    # Apply stemming\n",
            "    tokens = apply_stemming(tokens)\n",
            "\n",
            "    return tokens\n",
            "\n",
            "def handle_contractions(tokens):\n",
            "    # Common English contractions\n",
            "    contractions = {\n",
            "        \"can't\": \"can not\",\n",
            "        \"don't\": \"do not\",\n",
            "        \"is'nt\": \"is not\",\n",
            "        \"must'nt\":\"must not\",\n",
            "        \"wo'nt\": \"would not\"\n",
            "    }\n",
            "    # Iterate for all tokens, if the given token is in the keys of dictionary provided here then replace\n",
            "    # it with its corresponding value(expansion)\n",
            "    expanded_tokens = [contractions[token] if token in contractions else token for token in tokens]\n",
            "    return expanded_tokens\n",
            "\n",
            "def apply_stemming(tokens):\n",
            "    # Use a stemmer to reduce words to their root form\n",
            "    stemmer = PorterStemmer() #intialize\n",
            "    stemmed_tokens = [stemmer.stem(token) for token in tokens] #convert each token to stem word\n",
            "    return stemmed_tokens\n",
            "\n",
            "def create_sentiment_vectors(train_file):\n",
            "    # Load the training data\n",
            "    train_data = pd.read_csv(train_file)\n",
            "\n",
            "    # Initialize sentiment vectors dictionary\n",
            "    sentiment_vectors = {'positive': None, 'negative': None}\n",
            "\n",
            "    # Separate the data into positive and negative reviews\n",
            "    positive_reviews = train_data[train_data['sentiment'] == 'positive'] #positive\n",
            "    negative_reviews = train_data[train_data['sentiment'] == 'negative'] #negative\n",
            "\n",
            "    # Preprocess and tokenize reviews\n",
            "    positive_tokens = [preprocess_text(review) for review in positive_reviews['review']] \n",
            "    negative_tokens = [preprocess_text(review) for review in negative_reviews['review']]\n",
            "\n",
            "    # Calculate sentiment vectors(iterates for each word(token) in the review, if it has a corresponding embedding\n",
            "    #  in the Word2vec model the fetch that embedding. Then collect embedding for all words and average them\n",
            "    # separately for positive and negative sentiments.)\n",
            "    sentiment_vectors['positive'] = np.mean([model[word] for review_tokens in positive_tokens for word in review_tokens if word in model], axis=0)\n",
            "    sentiment_vectors['negative'] = np.mean([model[word] for review_tokens in negative_tokens for word in review_tokens if word in model], axis=0)\n",
            "\n",
            "    return sentiment_vectors\n",
            "# calling the function on train dataset(80% data here). Storing results in sent_vects\n",
            "sent_vects=create_sentiment_vectors(\"/content/train_IMDB_Dataset_20.csv\") \n",
            "\n",
            "def predict_sentiment(test_file, sent_vects):\n",
            "    # Load the test data\n",
            "    test_data = pd.read_csv(test_file)\n",
            "\n",
            "    # Initialize variables to track accuracy\n",
            "    total_predictions = 0\n",
            "    correct_predictions = 0\n",
            "\n",
            "    for _, row in test_data.iterrows():\n",
            "        # Preprocess and tokenize the test text\n",
            "        test_text = preprocess_text(row['review'])\n",
            "\n",
            "        # Calculate the embedding for the test text\n",
            "        test_embedding = np.mean([model[word] for word in test_text if word in model], axis=0)\n",
            "\n",
            "        # Initialize variables to track similarity scores\n",
            "        max_similarity = -1  # Initialize with a negative value\n",
            "        predicted_label = None\n",
            "\n",
            "        # Calculate cosine similarity between the test text embedding and each vector in sent_vects\n",
            "        for label, sentiment_vector in sent_vects.items():\n",
            "            similarity = cosine_similarity([test_embedding], [sentiment_vector])[0][0]\n",
            "\n",
            "            if similarity > max_similarity: # if similarity is greater than max_simlarity\n",
            "                max_similarity = similarity # then update value of max_similarity to be equal to similarity\n",
            "                predicted_label = label # that label becoms the predicted label\n",
            "\n",
            "        # Compare the predicted label with the actual label\n",
            "        if predicted_label == row['sentiment']: #if predicted label is equal to actual label\n",
            "            correct_predictions += 1 # increase correct predictions by 1\n",
            "\n",
            "        total_predictions += 1 # increase total _predictions by 1\n",
            "\n",
            "    # Calculate accuracy\n",
            "    accuracy = correct_predictions / total_predictions\n",
            "\n",
            "    # Print the final accuracy\n",
            "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
            "\n",
            "predict_sentiment(\"/content/test_IMDB_Dataset_20.csv\", sent_vects) #calling the function on test(20%) data\n",
            "\n",
            "# Function to preprocess and tokenize text\n",
            "def preprocess_text_improved(text):\n",
            "    tokens = word_tokenize(text) # tokenize each word\n",
            "    tokens = [word.lower() for word in tokens if word.isalnum()] #removes punctuation and convert to lowercase\n",
            "    # tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
            "    return tokens\n",
            "\n",
            "def create_better_vectors(train_file):\n",
            "    # Load the training data\n",
            "    train_data = pd.read_csv(train_file)\n",
            "\n",
            "    # Initialize sentiment vectors dictionary\n",
            "    sentiment_vectors = {'positive': None, 'negative': None}\n",
            "\n",
            "    # Separate the data into positive and negative reviews\n",
            "    positive_reviews = train_data[train_data['sentiment'] == 'positive'] #positive\n",
            "    negative_reviews = train_data[train_data['sentiment'] == 'negative'] #negative\n",
            "\n",
            "    # Preprocess and tokenize reviews (can be enhanced here)\n",
            "    positive_tokens = [preprocess_text_improved(review) for review in positive_reviews['review']]\n",
            "    negative_tokens = [preprocess_text_improved(review) for review in negative_reviews['review']]\n",
            "\n",
            "    # Calculate sentiment vectors using an improved approach\n",
            "    #(iterates for each word(token) in the review, if it has a corresponding embedding\n",
            "    #  in the Word2vec model the fetch that embedding. Then collect embedding for all words and average them\n",
            "    # separately for positive and negative sentiments.)\n",
            "    sentiment_vectors['positive'] = np.mean([model[word] for review_tokens in positive_tokens for word in review_tokens if word in model], axis=0)\n",
            "    sentiment_vectors['negative'] = np.mean([model[word] for review_tokens in negative_tokens for word in review_tokens if word in model], axis=0)\n",
            "\n",
            "    return sentiment_vectors\n",
            "#calling the function on train(80%) data\n",
            "better_sent_vectors=create_better_vectors(\"/content/train_IMDB_Dataset_20.csv\")\n",
            "\n",
            "def predict_better(test_file, sent_vects):\n",
            "    # Load the test data\n",
            "    test_data = pd.read_csv(test_file)\n",
            "\n",
            "    # Initialize variables to track accuracy\n",
            "    total_predictions = 0\n",
            "    correct_predictions = 0\n",
            "\n",
            "    for _, row in test_data.iterrows():\n",
            "        # Preprocess and tokenize the test text using the same preprocessing steps as create_better_vectors\n",
            "        test_text = preprocess_text_improved(row['review'])\n",
            "\n",
            "        # Calculate the embedding for the test text\n",
            "        test_embedding = np.mean([model[word] for word in test_text if word in model], axis=0)\n",
            "\n",
            "        # Initialize variables to track similarity scores\n",
            "        max_similarity = -1  # Initialize with a negative value\n",
            "        predicted_label = None\n",
            "\n",
            "        # Calculate cosine similarity between the test text embedding and each vector in sent_vects\n",
            "        for label, sentiment_vector in sent_vects.items():\n",
            "            similarity = cosine_similarity([test_embedding], [sentiment_vector])[0][0]\n",
            "\n",
            "            if similarity > max_similarity: \n",
            "                max_similarity = similarity # update the value of max similarity to be equal to similarity\n",
            "                predicted_label = label #update the predicted label\n",
            "\n",
            "        # Compare the predicted label with the actual label\n",
            "        if predicted_label == row['sentiment']: # if actual= predicted\n",
            "            correct_predictions += 1 #increase correct predictions\n",
            "\n",
            "        total_predictions += 1 # increase total predictions\n",
            "\n",
            "    # Calculate accuracy\n",
            "    accuracy = correct_predictions / total_predictions\n",
            "\n",
            "    # Print the final accuracy\n",
            "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
            "#calling function on test(20%) data.\n",
            "predict_better(\"/content/test_IMDB_Dataset_20.csv\",better_sent_vectors)\n",
            "\n",
            "report• The standard preprocessing used in all 4 methods. \n",
            "\tThere are two sets of preprocessing operations used in the 4 methods:\n",
            "\tFor create_sentiment_vectors and predict_sentiment functions, the preprocessing consists of word tokenization, conversion to lowercase,eliminating punctuations, handling contractions(Eg- “don’t” changed to “do not”) and applying stemming. \n",
            "\tFor create_better_vectors and predict_better functions, the preprocessing is kept simple with just word tokenization, eliminating punctuations and conversion to lowercase. \n",
            "• The pretrained embeddings used (link to external source). \n",
            "\tI have used Word2Vec embedding with the identifier “word2vec-google-news-300”. I used the gensim module to download the pretrained embeddings. \n",
            "\tThis link provided in Homework suggestions was used: https://radimrehurek.com/gensim/models/word2vec.html#pretrained-models)\n",
            "• The improvements tried in create_better_vectors. \n",
            "\tAfter various trials with different kind of preprocessing techniques including stopword removal, lemmatization etc, I concluded that simplicity gave best results. \n",
            "\tTherefore the preprocessing used in create_better_vectors preprocessing was kept simple with just word tokenization, eliminating punctuations and conversion to lowercase.\n",
            "• A table containing the accuracy for the first 2 methods (create_sentiment and predict_sentiment) as well as the improved accuracies achieved.\n",
            "Method\tAccuracy\n",
            "create_sentiment_vectors() and predict_sentiment()\t67.33%\n",
            "create_better_vectors() and predict_better()\t71.57%\n",
            "\n",
            "• Discussion on the results achieve. Surprises, difficulties, specific additions which helped improve accuracy or hurt accuracy\n",
            "\tWhat was surprising was that the application of more sophisticated preprocessing techniques only reduced the accuracy. \n",
            "\tUsing stopwords removal posed some difficulty as it was taking too long to finish therefore I had to remove that from the preprocessing steps. \n",
            "\tIn the end, I went forward with simple and basic preprocessing steps including word tokenization, eliminating punctuations and conversion to lowercase.\n",
            "\n",
            "task_expectationsHomework Reports\n",
            "\n",
            "Things your report might include:\n",
            "\n",
            "Discussion/analysis of results (just noting results is not a discussion).\n",
            "\n",
            "Reasoning behind your algorithms explored. ([if required] for example in text normalization, why did you\n",
            "\n",
            "choose those normalizations)\n",
            "\n",
            "Any downsides or things your algorithms miss. (found by either experiments or what you believe is missing). • Each homework notes what should be in the report, so make sure you know what it requires for each assignment.\n",
            "\n",
            "Things your report should not include:\n",
            "\n",
            "• Your reports should not have screenshots of code, this is irrelevant, especially since you are handing in code.\n",
            "\n",
            "Screenshots of results (take the time to add in tables or diagrams to present results for that specific document. E.g. tables in word documents).\n",
            "\n",
            "Explanations of all code choices. Your report is not there to describe how you coded your assignment.\n",
            "Python Code\n",
            "\n",
            "• Code should have documentation, including comments\n",
            "\n",
            "• Code should define the functions specified in the homeworks (You may define other functions to assist, but the require functions should work as outlined)\n",
            "\n",
            "Jupyter notebooks are not adequate as python files. You may use jupyter notebook for your experiments, but take the time to extract the necessary code for your python files and results for your report.\n",
            "\n",
            "Any larger chunks of code taken from online should be cited via a link as a comment.\n",
            "\n",
            "Never share your code with others, plagiarized homeworks result in automatic O's. Come see me if you need help.\n",
            "\n",
            "Provide a feedback:\n",
            "\n",
            "Correct answer : Overall Feedback\n",
            "-2 (Report is missing discussions on improvements or degradations of accuracy when you tried updating create_better_vectors)\n",
            "Model's answer: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 514, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-c514ca14de93>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Correct answer : {split_dataset[test_dataset][0]['output']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model's answer: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-f962ccd19183>\u001b[0m in \u001b[0;36minference\u001b[0;34m(text, model, tokenizer, max_input_tokens, max_output_tokens)\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;31m# Generate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   generated_tokens_with_prompt = model.generate(\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_output_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1671\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgeneration_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mGenerationMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGREEDY_SEARCH\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1672\u001b[0m             \u001b[0;31m# 11. run greedy search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1673\u001b[0;31m             return self.greedy_search(\n\u001b[0m\u001b[1;32m   1674\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1675\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgreedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2520\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2521\u001b[0;31m             outputs = self(\n\u001b[0m\u001b[1;32m   2522\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2523\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0muse_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 953\u001b[0;31m         outputs = self.roberta(\n\u001b[0m\u001b[1;32m    954\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    826\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m         embedding_output = self.embeddings(\n\u001b[0m\u001b[1;32m    829\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mposition_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0membeddings\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mposition_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         return F.layer_norm(\n\u001b[0m\u001b[1;32m    197\u001b[0m             input, self.normalized_shape, self.weight, self.bias, self.eps)\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2541\u001b[0m             \u001b[0mlayer_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2542\u001b[0m         )\n\u001b[0;32m-> 2543\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_steps = 2\n",
        "trained_model_name = f\"my_squad_{max_steps}_steps\"\n",
        "output_dir = trained_model_name"
      ],
      "metadata": {
        "id": "H1L8mcKmE2Wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "\n",
        "  # Learning rate\n",
        "  learning_rate=1.0e-5,\n",
        "\n",
        "  # Number of training epochs\n",
        "  num_train_epochs=2,\n",
        "\n",
        "  # Max steps to train for (each step is a batch of data)\n",
        "  # Overrides num_train_epochs, if not -1\n",
        "  max_steps=-1,\n",
        "\n",
        "  # Batch size for training\n",
        "  per_device_train_batch_size=1,\n",
        "\n",
        "  # Directory to save model checkpoints\n",
        "  output_dir=output_dir,\n",
        "\n",
        "  # Other arguments\n",
        "  overwrite_output_dir=False, # Overwrite the content of the output directory\n",
        "  disable_tqdm=False, # Disable progress bars\n",
        "  eval_steps=60, # Number of update steps between two evaluations\n",
        "  save_steps=60, # After # steps model is saved\n",
        "  warmup_steps=1, # Number of warmup steps for learning rate scheduler\n",
        "  per_device_eval_batch_size=1, # Batch size for evaluation\n",
        "  evaluation_strategy=\"steps\",\n",
        "  logging_strategy=\"steps\",\n",
        "  logging_steps=1,\n",
        "  optim=\"adafactor\",\n",
        "  gradient_accumulation_steps = 4,\n",
        "  gradient_checkpointing=False,\n",
        "\n",
        "  # Parameters for early stopping\n",
        "  load_best_model_at_end=True,\n",
        "  save_total_limit=1,\n",
        "  metric_for_best_model=\"eval_loss\",\n",
        "  greater_is_better=False\n",
        ")"
      ],
      "metadata": {
        "id": "ShO4ODIUE2c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_flops = (\n",
        "#   base_model.floating_point_ops(\n",
        "#     {\n",
        "#        \"input_ids\": torch.zeros(\n",
        "#            (1, training_config[\"model\"][\"max_length\"])\n",
        "#       )\n",
        "#     }\n",
        "#   )\n",
        "#   * training_args.gradient_accumulation_steps\n",
        "# )\n",
        "\n",
        "# print(base_model)\n",
        "# print(\"Memory footprint\", base_model.get_memory_footprint() / 1e9, \"GB\")\n",
        "# print(\"Flops\", model_flops / 1e9, \"GFLOPs\")"
      ],
      "metadata": {
        "id": "s8Fi4I6FK8Sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=split_dataset[train_dataset]\n",
        "test_dataset=split_dataset[test_dataset]"
      ],
      "metadata": {
        "id": "mdVrXsw_LXKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import Trainer\n",
        "# from utilities import *\n",
        "# !pip install transformers\n",
        "\n"
      ],
      "metadata": {
        "id": "5NpyW391L7YN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ],
      "metadata": {
        "id": "hcMpEpwiQg5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=base_model,\n",
        "    # model_flops=model_flops,\n",
        "    # total_steps=100,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset\n",
        ")"
      ],
      "metadata": {
        "id": "Ywm7MLFdK8f1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "id": "B5VqYE7IiR6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_output = trainer.train()"
      ],
      "metadata": {
        "id": "-ljRMfvcOnG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_output"
      ],
      "metadata": {
        "id": "FjEnB41Bo67i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_dir = f'{output_dir}/finetuned_2'\n",
        "\n",
        "trainer.save_model(save_dir)\n",
        "print(\"Saved model to:\", save_dir)"
      ],
      "metadata": {
        "id": "hLT93JVwOUZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetuned_2 = AutoModelForCausalLM.from_pretrained(save_dir, local_files_only=True)\n"
      ],
      "metadata": {
        "id": "Y-eQSvBxOUob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetuned_2.to(device)\n"
      ],
      "metadata": {
        "id": "s-0_VT3vOU8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_question = test_dataset[1]['input']\n",
        "print(\"Question input (test):\", test_question)\n",
        "\n",
        "print(\"Finetuned_2 model's answer: \")\n",
        "print(inference(test_question, finetuned_2, tokenizer))"
      ],
      "metadata": {
        "id": "fibujrknO8NT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_answer = test_dataset[1]['output'][\"text\"][0]\n",
        "print(\"Target answer output (test):\", test_answer)"
      ],
      "metadata": {
        "id": "wyeikP3wO8Yw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rEQBX5DlO80W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KJM6_ZW8O9BD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}