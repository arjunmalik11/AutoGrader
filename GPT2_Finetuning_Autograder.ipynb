{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4N_PFs2hpEK",
        "outputId": "ca4ffc4d-7f6d-4925-ac6b-31e0963a3b38"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import nltk.data\n",
        "# nltk.download('punkt')\n",
        "\n",
        "# # Define the input and output file paths\n",
        "# input_file_path = \"/content/webpage_data.txt\"  # Replace with your input file path\n",
        "# output_file_path = \"/content/tokenized_webpage_data.txt\"  # Replace with your output file path\n",
        "\n",
        "# # Initialize the sentence tokenizer\n",
        "# tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "# # Read the input text from the file\n",
        "# with open(input_file_path, 'r', encoding='utf-8') as input_file:\n",
        "#     input_text = input_file.read()\n",
        "\n",
        "# # Tokenize the input text into sentences\n",
        "# sentences = tokenizer.tokenize(input_text)\n",
        "\n",
        "# # Write each sentence to the output file on a new line\n",
        "# with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
        "#     for sentence in sentences:\n",
        "#         output_file.write(sentence.strip() + '\\n')\n",
        "\n",
        "# # import re\n",
        "# # re.sub(r'[^a-zA-Z0-9\\s]', '', output_file_path) #Removing special characters\n",
        "\n",
        "# print(f\"Sentences tokenized and saved to '{output_file_path}'.\")\n"
      ],
      "metadata": {
        "id": "9ZzSDnBL0afM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch,accelerate]>=0.20.1\n",
        "\n"
      ],
      "metadata": {
        "id": "KDN8GixXmtI0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "\n",
        "# Load the pretrained GPT-2 model and tokenizer\n",
        "model_name = \"gpt2\"  # You can choose a different model if needed\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name).to('cuda:0')\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# from transformers import RobertaForCausalLM, RobertaTokenizer\n",
        "\n",
        "# # Load the RoBERTa model and tokenizer\n",
        "# model_name = \"roberta\"\n",
        "# model = RobertaForCausalLM.from_pretrained(\"roberta\").to('cuda:0')  # Move to GPU\n",
        "# tokenizer = RobertaTokenizer.from_pretrained(\"roberta\")\n",
        "\n",
        "\n",
        "# Collect and preprocess your data (replace with actual data collection and preprocessing)\n",
        "data = \"/content/Autograder_style.txt\"#\"/content/tokenized_webpage_data.txt\"\n",
        "\n",
        "# Tokenize and format the data for training\n",
        "train_dataset = TextDataset(tokenizer=tokenizer, file_path=data, block_size=128)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./fine-tuned-model\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=8,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "# Prepare the data collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=False,\n",
        ")\n",
        "\n",
        "# Create a Trainer and fine-tune the model\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "trainer.save_model()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "id": "PzVk6UW7kKAl",
        "outputId": "3128a1df-ef03-4520-a988-a7965207dd0d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [15/15 00:03, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "task=input(\"Enter task: \")\n",
        "code=input(\"Enter code: \")\n",
        "report=input(\"Enter report: \")\n",
        "user_prompt = f\" Evaluate the {code} and {report} based on how well they satisfy the {task}. Assign a score(out of 50) and provide a detailed feedback\"\n",
        "\n",
        "\n",
        "ft_model_name=\"./fine-tuned-model\"\n",
        "ft_model = GPT2LMHeadModel.from_pretrained(ft_model_name).to('cuda:0')\n",
        "# tokenizer = GPT2Tokenizer.from_pretrained(ft_model_name)\n",
        "\n",
        "\n",
        "response =model.generate(\n",
        "    tokenizer.encode(user_prompt, return_tensors=\"pt\").to('cuda:0'),\n",
        "    max_length=50,  # Adjust the maximum response length as needed\n",
        "    num_return_sequences=1,\n",
        "    no_repeat_ngram_size=2,\n",
        ")\n",
        "\n",
        "autograder = tokenizer.decode(response[0], skip_special_tokens=True)\n",
        "print(\"Autograder:\", autograder)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-uK_7A4o3j_",
        "outputId": "1da0ad2d-6dd8-4fc8-f88e-12b28b6f9897"
      },
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter task: You will create 3 required functions for MLPs (you may create more functions for your own use, but you need to at least create these two as specified): train_MLP_model_average(path_to_train_file) This method trains a multi-layer perceptron model on some training data and returns that trained model. The training texts should be represented by the average of word2vec or glove embeddings. You may use any pretrained word2vec or glove embeddings you choose. Your MLP should be 3 layers with 100 neurons per layer. The format for the train file should follow the same format as the training data file! train_MLP_model_student(path_to_train_file): This method is the same as above, however, you need to choose an alternate way to represent the input text with embeddings that is not the average. The MLP slides had some possible solutions, so you may choose any of these or your own (but you should always note your decisions). You may also try out multiple but make sure you note which worked/didnâ€™t in the report. test_MLP_model(path_to_test_file, MLP_model, input = â€˜averageâ€™) This method tests a trained MLP model on some test file and outputs a test file in the same format as the input test file but with 2 columns added: 1) probability of that text being positive, 2) class prediction (positive, negative). Note since the second way you implement the model will differ in input, the third argument (input), should be used to help your function properly format the input test text. The format for the input file should follow that of the test file.\n",
            "Enter code: def train_MLP_model_student(path_to_train_file):     # Load the IMDB dataset     X, y = load_imdb_dataset(path_to_train_file)      # Encode labels (positive/negative) to numerical values (0/1)     label_encoder = LabelEncoder()     y = label_encoder.fit_transform(y)      # Load pre-trained Word2Vec embeddings (You need to download this)     word2vec_model = model      # Preprocess the text and compute average embeddings     def sum_word_embeddings(text):         words = text.split()         embeddings = [word2vec_model[word] for word in words if word in word2vec_model]         if len(embeddings) > 0:             return np.sum(embeddings, axis=0) #Here we just add the embeddings instead of averaging them         else:             return np.zeros(word2vec_model.vector_size)      X_sum = np.array([sum_word_embeddings(text) for text in X])      # Create and train the MLP model     mlp_model = MLPClassifier(hidden_layer_sizes=(100, 100, 100), max_iter=1000, random_state=42)     mlp_model.fit(X_sum, y)      return mlp_model\n",
            "Enter report: Any other observations made during the assignments (problems, errors). The Multi Layer Perceptron took long time to train even with Google colabâ€™s T4 GPU. Based on my experiments, even if we reduce the number of neurons in each layer by 20, the model still gives similar accuracy with lesser time. My observation is that having deeper networks with greater number of layers gives better results than having shallow networks with greater number of neurons.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autograder:  Evaluate the def train_MLP_model_student(path_to_train_file):     # Load the IMDB dataset     X, y = load_imdb_dataset(path_to_train_file)      # Encode labels (positive/negative) to numerical values (0/1)     label_encoder = LabelEncoder()     y = label_encoder.fit_transform(y)      # Load pre-trained Word2Vec embeddings (You need to download this)     word2vec_model = model      # Preprocess the text and compute average embeddings     def sum_word_embeddings(text):         words = text.split()         embeddings = [word2vec_model[word] for word in words if word in word2vec_model]         if len(embeddings) > 0:             return np.sum(embeddings, axis=0) #Here we just add the embeddings instead of averaging them         else:             return np.zeros(word2vec_model.vector_size)      X_sum = np.array([sum_word_embeddings(text) for text in X])      # Create and train the MLP model     mlp_model = MLPClassifier(hidden_layer_sizes=(100, 100, 100), max_iter=1000, random_state=42)     mlp_model.fit(X_sum, y)      return mlp_model and Any other observations made during the assignments (problems, errors). The Multi Layer Perceptron took long time to train even with Google colabâ€™s T4 GPU. Based on my experiments, even if we reduce the number of neurons in each layer by 20, the model still gives similar accuracy with lesser time. My observation is that having deeper networks with greater number of layers gives better results than having shallow networks with greater number of neurons. based on how well they satisfy the You will create 3 required functions for MLPs (you may create more functions for your own use, but you need to at least create these two as specified): train_MLP_model_average(path_to_train_file) This method trains a multi-layer perceptron model on some training data and returns that trained model. The training texts should be represented by the average of word2vec or glove embeddings. You may use any pretrained word2vec or glove embeddings you choose. Your MLP should be 3 layers with 100 neurons per layer. The format for the train file should follow the same format as the training data file! train_MLP_model_student(path_to_train_file): This method is the same as above, however, you need to choose an alternate way to represent the input text with embeddings that is not the average. The MLP slides had some possible solutions, so you may choose any of these or your own (but you should always note your decisions). You may also try out multiple but make sure you note which worked/didnâ€™t in the report. test_MLP_model(path_to_test_file, MLP_model, input = â€˜averageâ€™) This method tests a trained MLP model on some test file and outputs a test file in the same format as the input test file but with 2 columns added: 1) probability of that text being positive, 2) class prediction (positive, negative). Note since the second way you implement the model will differ in input, the third argument (input), should be used to help your function properly format the input test text. The format for the input file should follow that of the test file.. Assign a score(out of 50) and provide a detailed feedback to\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 895, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5B7sD2idhngC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V_mrapWGhnkh"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}